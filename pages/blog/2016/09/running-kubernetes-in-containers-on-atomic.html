{{define "title"}}Running Kubernetes and Friends in Containers on CentOS Atomic Host{{end}}
{{define "body"}}

<article class='post hentry'>
<header class='post-header'>
<h2 class='post-title entry-title'>
Running Kubernetes and Friends in Containers on CentOS Atomic Host
</h2>
<p class='post-meta'>
<span class='byline'>
by
</span>
<span class='author vcard'>
<a href="/blog/author/jbrooks/">Jason Brooks</a>
</span>
&ndash;
<time class='published' datetime='2016-09-15T12:00:00Z'>
Thursday 15 September 2016
</time>
</p>
</header>
<section class='post-content entry-content'>
<p>The <a href="http://www.projectatomic.io/docs/introduction/">atomic hosts</a> from CentOS and Fedora earn their <q>atomic</q> namesake by providing for atomic, image-based system updates via rpm-ostree, and atomic, image-based application updates via docker containers.</p>

<p>This <q>system</q> vs <q>application</q> division isn&rsquo;t set in stone, however. There&rsquo;s room for system components to move across from the <a href="http://www.projectatomic.io/blog/2016/07/hacking-and-extending-atomic-host/">somewhat</a> rigid world of ostree commits to the freer-flowing container side.</p>

<p>In particular, the key atomic host components involved in orchestrating containers across multiple hosts, such as flannel, etcd and kubernetes, could run instead in containers, making life simpler for those looking to test out newer or different versions of these components, or to swap them out for alternatives.</p>

<p>The <a href="https://wiki.centos.org/SpecialInterestGroup/Atomic/Devel">devel tree</a> of CentOS Atomic Host, which features a trimmed-down system image that leaves out kubernetes and related system components, is a great place to experiment with alternative methods of running these components, and swapping between them.</p>

<p></p>

<h3>System Containers</h3>

<p>Running system components in docker containers can be tricky, because these containers aren&rsquo;t automatically integrated with systemd, like other system services, and because some components, such as flannel, need to modify docker configs, resulting in a bit of a chicken-and-egg situation.</p>

<p>One solution is <a href="http://www.projectatomic.io/blog/2016/09/intro-to-system-containers/">system containers for atomic</a>, which can be run independently from the docker daemon. <a href="https://twitter.com/gscrivano">Giuseppe Scrivano</a> has built example containers <a href="https://hub.docker.com/r/gscrivano/flannel/">for flannel</a> and <a href="https://hub.docker.com/r/gscrivano/etcd/">for etcd</a>, and in this post, I&rsquo;ll be using system containers to run flannel and etcd on my atomic hosts.</p>

<p>You need a very recent version of the <code>atomic</code> command. I used a pair of CentOS Atomic Hosts running the <a href="https://wiki.centos.org/SpecialInterestGroup/Atomic/Devel"><q>continuous</q></a> stream.</p>

<p>The master host needs etcd and flannel:</p>
<pre class="highlight plaintext"><code># atomic install --system gscrivano/etcd&#x000A;&#x000A;# systemctl start etcd&#x000A;</code></pre>

<p>With etcd running, we can use it to configure flannel:</p>
<pre class="highlight plaintext"><code># runc exec etcd etcdctl set /atomic.io/network/config '{"Network":"172.17.0.0/16"}'&#x000A;&#x000A;# atomic install --system gscrivano/flannel&#x000A;&#x000A;# systemctl start flannel&#x000A;</code></pre>

<p>The worker node needs flannel as well:</p>
<pre class="highlight plaintext"><code># export MASTER_IP=YOUR-MASTER-IP&#x000A;&#x000A;# atomic install --system --set FLANNELD_ETCD_ENDPOINTS=http://$MASTER_IP:2379 gscrivano/flannel&#x000A;&#x000A;# systemctl start flannel&#x000A;</code></pre>

<p>On both the master and the worker, we need to make docker use flannel:</p>
<pre class="highlight plaintext"><code># echo "/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker" | runc exec flannel bash&#x000A;</code></pre>

<p>Also on both hosts, we need this docker tweak (<a href="https://github.com/kubernetes/kubernetes/issues/4869">because of this</a>):</p>
<pre class="highlight plaintext"><code># cp /usr/lib/systemd/system/docker.service /etc/systemd/system/&#x000A;&#x000A;# sed -i s/MountFlags=slave/MountFlags=/g /etc/systemd/system/docker.service&#x000A;&#x000A;# systemctl daemon-reload&#x000A;&#x000A;# systemctl restart docker&#x000A;</code></pre>

<p>On both hosts, some context tweaks to make SELinux happy:</p>
<pre class="highlight plaintext"><code># mkdir -p /var/lib/kubelet/&#x000A;&#x000A;# chcon -R -t svirt_sandbox_file_t /var/lib/kubelet/&#x000A;&#x000A;# chcon -R -t svirt_sandbox_file_t /var/lib/docker/&#x000A;</code></pre>

<h3>Kube in Containers</h3>

<p>With etcd and flannel in place, we can proceed with running kubernetes. We can run each of the kubernetes master and worker components in containers, using the rpms available in the CentOS repositories. Here I&rsquo;m using containers built from <a href="https://github.com/jasonbrooks/CentOS-Dockerfiles/tree/pr-kubernetes/kubernetes">these dockerfiles</a>.</p>

<h4>On the master</h4>
<pre class="highlight plaintext"><code># export MASTER_IP=YOUR-MASTER-IP&#x000A;&#x000A;# docker run -d --net=host jasonbrooks/kubernetes-apiserver:centos --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota --address=0.0.0.0 --insecure-bind-address=0.0.0.0&#x000A;&#x000A;# docker run -d --net=host --privileged jasonbrooks/kubernetes-controller-manager:centos&#x000A;&#x000A;# docker run -d --net=host jasonbrooks/kubernetes-scheduler:centos&#x000A;</code></pre>

<h4>On the worker</h4>
<pre class="highlight plaintext"><code># export WORKER_IP=YOUR-WORKER-IP&#x000A;&#x000A;# atomic run --opt3="--master=http://$MASTER_IP:8080" jasonbrooks/kubernetes-proxy:centos&#x000A;&#x000A;# atomic run --opt1="-v /etc/kubernetes/manifests:/etc/kubernetes/manifests:ro" --opt3="--address=$WORKER_IP --config=/etc/kubernetes/manifests --hostname_override=$WORKER_IP --api_servers=http://$MASTER_IP:8080 --cluster-dns=10.254.0.10 --cluster-domain=cluster.local" jasonbrooks/kubernetes-kubelet:centos&#x000A;</code></pre>

<h4>Get matching kubectl</h4>

<p>I like to test things out from my master node. We can grab the version of the kubernetes command line client, kubectl, that matches our rpms by extracting it from the CentOS rpm.</p>
<pre class="highlight plaintext"><code># rpm2cpio http://mirror.centos.org/centos/7/extras/x86_64/Packages/kubernetes-client-1.2.0-0.13.gitec7364b.el7.x86_64.rpm | cpio -iv --to-stdout "./usr/bin/kubectl" &gt; /usr/local/bin/kubectl &amp;&amp; chmod +x /usr/local/bin/kubectl&#x000A;</code></pre>

<p>We can then use kubectl to check on the status of our node(s):</p>
<pre class="highlight plaintext"><code># kubectl get nodes&#x000A;&#x000A;NAME            STATUS    AGE&#x000A;10.10.171.216   Ready     3h&#x000A;</code></pre>

<h4>DNS Addon</h4>

<p>The guestbookgo sample app that I like to use for testing requires the dns addon, so I always ensure that it&rsquo;s installed. Here I&rsquo;m following the directions from the docker-multinode page in the kube-deploy repository:</p>
<pre class="highlight plaintext"><code># curl -O https://raw.githubusercontent.com/kubernetes/kube-deploy/master/docker-multinode/skydns.yaml&#x000A;</code></pre>

<p>I skipped over setting up certificates for this cluster, so in order for the dns addon to work, the kube2sky container in the dns pod needs the argument <code>--kube-master-url=http://$MASTER_IP:8080</code>. Also, I&rsquo;m changing the <code>clusterIP</code> to <code>10.254.0.10</code> to match the default from the rpms.</p>
<pre class="highlight plaintext"><code># vi skydns.yaml&#x000A;&#x000A;...&#x000A;&#x000A;- name: kube2sky&#x000A;        image: gcr.io/google_containers/kube2sky-ARCH:1.15&#x000A;        resources:&#x000A;          limits:&#x000A;            cpu: 100m&#x000A;            memory: 50Mi&#x000A;        args:&#x000A;        # command = "/kube2sky"&#x000A;        - --domain=cluster.local&#x000A;        - --kube-master-url=http://$MASTER_IP:8080&#x000A;...&#x000A;&#x000A;spec:&#x000A;  selector:&#x000A;    k8s-app: kube-dns&#x000A;  clusterIP: 10.254.0.10&#x000A;&#x000A;</code></pre>

<p>Now to start up the dns addon:</p>
<pre class="highlight plaintext"><code># kubectl create namespace kube-system&#x000A;&#x000A;# export ARCH=amd64&#x000A;&#x000A;# sed -e "s/ARCH/${ARCH}/g;" skydns.yaml | kubectl create -f -&#x000A;</code></pre>

<h3>Test it</h3>

<p>It takes a few minutes for all the containers to get up and running. Once they are, you can start running kubernetes apps. I typically test with the <a href="https://github.com/projectatomic/nulecule-library/tree/master/guestbookgo-atomicapp">guestbookgo atomicapp</a>:</p>
<pre class="highlight plaintext"><code># atomic run projectatomic/guestbookgo-atomicapp&#x000A;</code></pre>

<p>Wait a few minutes, until <code>kubectl get pods</code> tells you that your guestbook and redis pods are running, and then:</p>
<pre class="highlight plaintext"><code># kubectl describe service guestbook | grep NodePort&#x000A;</code></pre>

<p>Visiting the <code>NodePort</code> returned above at either my master or worker IP (these kube scripts configure both to serve as workers) gives me this:</p>

<p><img src="/images/guestbook-ftw.png" /></p>

<h3>A Newer Kube</h3>

<p>The kubernetes rpms in the CentOS repositories are currently at version 1.2. To try out the current, 1.3 version of kubernetes, you can swap in some newer rpms, running in Fedora or Rawhide-based containers, or you can use the latest kubernetes containers provided by the upstream project.</p>

<h4>Cleaning up</h4>

<p>If you&rsquo;ve already configured kubernetes using the above instructions, and want to start over with a different kubernetes version, you can blow your existing cluster away (while leaving the flannel and etcd pieces in place).</p>

<p>First, remove all the docker containers running on your master and node(s). On each machine, run:</p>
<pre class="highlight plaintext"><code># docker rm -f $(docker ps -a -q)&#x000A;</code></pre>

<p>Run <code>docker ps</code> to see if any containers are left over, if there are, run the command above again until they&rsquo;re all gone. Since we&rsquo;re running flannel and etcd using runc, killing all our docker containers won&rsquo;t affect our system containers.</p>

<p>Next, you can clear out the etcd keys associated with the cluster by running this on your master:</p>
<pre class="highlight plaintext"><code># runc exec -t etcd etcdctl rm --recursive /registry&#x000A;</code></pre>

<p>Then, on your worker node, clean up the <code>/var/lib/kubelet</code> directory:</p>
<pre class="highlight plaintext"><code># rm -rf /var/lib/kubelet/*&#x000A;</code></pre>

<h3>Containers from Rawhide</h3>

<p>I mentioned that the CentOS repositories contain a v1.2 kubernetes. However, Fedora&rsquo;s <a href="https://fedoraproject.org/wiki/Releases/Rawhide">Rawhide</a> includes v1.3-based kubernetes packages. howI wanted to try out the most recent rpm-packaged kubernetes, so I rebuilt the containers I used above swapping in <code>fedora:rawhide</code> for <code>centos:centos7</code> in the <code>FROM:</code> lines of the dockerfiles.</p>

<p>You can run the same series of commands listed above, with the container tag changed from <code>:centos</code> to <code>:rawhide</code>.</p>

<p>For instance:</p>
<pre class="highlight plaintext"><code># docker run -d --net=host jasonbrooks/kubernetes-scheduler:centos --master=http://$MASTER_IP:8080&#x000A;</code></pre>

<p>becomes:</p>
<pre class="highlight plaintext"><code># docker run -d --net=host jasonbrooks/kubernetes-scheduler:rawhide --master=http://$MASTER_IP:8080&#x000A;</code></pre>

<h3>Containers from Upstream</h3>

<p>With flannel and etcd running in system containers, and with docker configured properly, we can start up kubernetes using <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/images/hyperkube">the containers</a> built by the upstream kubernetes project. I&rsquo;ve pulled the following docker run commands from the <a href="https://github.com/kubernetes/kube-deploy/tree/master/docker-multinode">docker-multinode</a> scripts in the kubernetes project&rsquo;s kube-deploy repository.</p>

<p>On the master:</p>
<pre class="highlight plaintext"><code># docker run -d \&#x000A;    --net=host \&#x000A;    --pid=host \&#x000A;    --privileged \&#x000A;    --restart="unless-stopped" \&#x000A;    --name kube_kubelet_$(date | md5sum | cut -c-5) \&#x000A;    -v /sys:/sys:rw \&#x000A;    -v /var/run:/var/run:rw \&#x000A;    -v /run:/run:rw \&#x000A;    -v /var/lib/docker:/var/lib/docker:rw \&#x000A;    -v /var/lib/kubelet:/var/lib/kubelet:shared \&#x000A;    -v /var/log/containers:/var/log/containers:rw \&#x000A;    gcr.io/google_containers/hyperkube-amd64:$(curl -sSL "https://storage.googleapis.com/kubernetes-release/release/stable.txt") \&#x000A;    /hyperkube kubelet \&#x000A;      --allow-privileged \&#x000A;      --api-servers=http://localhost:8080 \&#x000A;      --config=/etc/kubernetes/manifests-multi \&#x000A;      --cluster-dns=10.0.0.10 \&#x000A;      --cluster-domain=cluster.local \&#x000A;      --hostname-override=${MASTER_IP} \&#x000A;      --v=2&#x000A;</code></pre>

<p>On the worker:</p>
<pre class="highlight plaintext"><code># export WORKER_IP=YOUR-WORKER-IP&#x000A;&#x000A;# docker run -d \&#x000A;    --net=host \&#x000A;    --pid=host \&#x000A;    --privileged \&#x000A;    --restart="unless-stopped" \&#x000A;    --name kube_kubelet_$(date | md5sum | cut -c-5) \&#x000A;    -v /sys:/sys:rw \&#x000A;    -v /var/run:/var/run:rw \&#x000A;    -v /run:/run:rw \&#x000A;    -v /var/lib/docker:/var/lib/docker:rw \&#x000A;    -v /var/lib/kubelet:/var/lib/kubelet:shared \&#x000A;    -v /var/log/containers:/var/log/containers:rw \&#x000A;    gcr.io/google_containers/hyperkube-amd64:$(curl -sSL "https://storage.googleapis.com/kubernetes-release/release/stable.txt") \&#x000A;    /hyperkube kubelet \&#x000A;      --allow-privileged \&#x000A;      --api-servers=http://${MASTER_IP}:8080 \&#x000A;      --cluster-dns=10.0.0.10 \&#x000A;      --cluster-domain=cluster.local \&#x000A;      --hostname-override=${WORKER_IP} \&#x000A;      --v=2&#x000A;&#x000A;# docker run -d \&#x000A;    --net=host \&#x000A;    --privileged \&#x000A;    --name kube_proxy_$(date | md5sum | cut -c-5) \&#x000A;    --restart="unless-stopped" \&#x000A;    gcr.io/google_containers/hyperkube-amd64:$(curl -sSL "https://storage.googleapis.com/kubernetes-release/release/stable.txt") \&#x000A;    /hyperkube proxy \&#x000A;        --master=http://${MASTER_IP}:8080 \&#x000A;        --v=2&#x000A;</code></pre>

<h4>Get current kubectl</h4>

<p>I usually test things out from the master node, so I&rsquo;ll download the newest stable kubectl binary to there:</p>
<pre class="highlight plaintext"><code># curl -sSL https://storage.googleapis.com/kubernetes-release/release/$(curl -sSL "https://storage.googleapis.com/kubernetes-release/release/stable.txt")/bin/linux/amd64/kubectl &gt; /usr/local/bin/kubectl&#x000A;&#x000A;# chmod +x /usr/local/bin/kubectl&#x000A;&#x000A;# kubectl get nodes&#x000A;</code></pre>

<p>From here, you an scroll up to the subhead <q>Test it</q> to run the guestbookgo  app. It&rsquo;s not necessary to start up the dns addon manually with these upstream containers, because this configuration starts that addon automatically.</p>

</section>
<footer class='post-meta'>
<div class='tags'>
Tags:
<ul class='taglist'>
<li><a href="/blog/tag/atomic/">atomic</a></li>
<li><a href="/blog/tag/centos/">centos</a></li>
<li><a href="/blog/tag/docker/">docker</a></li>
<li><a href="/blog/tag/kubernetes/">kubernetes</a></li>
<li><a href="/blog/tag/system-containers/">system containers</a></li>
</ul>
</div>

<p>
<small>Share this post:</small>
<span class='btn-group' id='social_share'>
<a class='btn btn-default btn-xs' href='http://twitter.com/share?text=yourtext&amp;url=http://www.projectatomic.io/blog/2016/09/running-kubernetes-in-containers-on-atomic/' role='button' title='Twitter'>
<i class='fa fa-twitter'></i>
Twitter
</a>
<a class='btn btn-default btn-xs' href='http://www.facebook.com/sharer.php?u=http://www.projectatomic.io/blog/2016/09/running-kubernetes-in-containers-on-atomic/' role='button' title='Facebook'>
<i class='fa fa-facebook'></i>
Facebook
</a>
<a class='btn btn-default btn-xs' href='https://plus.google.com/share?url=http://www.projectatomic.io/blog/2016/09/running-kubernetes-in-containers-on-atomic/' role='button' title='Google'>
<i class='fa fa-google-plus'></i>
Google+
</a>
</span>
</p>

</footer>
</article>

{{end}}