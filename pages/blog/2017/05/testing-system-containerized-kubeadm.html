{{define "title"}}Testing System-Containerized Kubeadm{{end}}
{{define "body"}}

<article class='post hentry'>
<header class='post-header'>
<h2 class='post-title entry-title'>
Testing System-Containerized Kubeadm
</h2>
<p class='post-meta'>
<span class='byline'>
by
</span>
<span class='author vcard'>
<a href="/blog/author/jbrooks/">Jason Brooks</a>
</span>
&ndash;
<time class='published' datetime='2017-05-30T13:00:00Z'>
Tuesday 30 May 2017
</time>
</p>
</header>
<section class='post-content entry-content'>
<p>Recently, I’ve been experimenting with running <a href="http://www.projectatomic.io/blog/2017/05/system-containerized-kube/">Kubernetes in system containers</a>, and those tests led me to wonder whether I could use system containers as a means of working around the <a href="https://jebpages.com/2016/11/01/installing-kubernetes-on-centos-atomic-host-with-kubeadm/">issues I’ve experienced</a> installing <a href="https://kubernetes.io/docs/getting-started-guides/kubeadm/">kubeadm</a>, the simple-to-use tool for bootstrapping kubernetes clusters, on an atomic host.</p>

<p>On a regular CentOS or Fedora host, using kubeadm is a matter of installing rpms for the kubelet, kubectl, kubeadm itself, and for a set of Kubernetes networking tools, kubernetes-cni. On an atomic host, rpm-ostree package layering allows for installing rpms, but if existing kube rpms are already part the atomic host image, as they are for Fedora Atomic Host, you won’t be able to install the prescribed upstream kube versions. And even on a host without built-in kubernetes, like <a href="https://wiki.centos.org/SpecialInterestGroup/Atomic/Devel">CentOS Atomic Continuous</a>, rpm-ostree won’t abide rpm content stored in <code>/opt</code>.</p>

<p></p>

<p>I managed to make a <a href="https://github.com/jasonbrooks/atomic-system-containers/tree/kube-containers/kubeadm">kubadm system container</a> that uses the same tmpfiles.d trick I used to link kubectl and etcdctl from the container into the the host’s <code>/usr/local/bin</code> to make the kubeadm tool available from the host.</p>

<p>For now, this works best on CentOS Atomic Host, with either the <a href="https://wiki.centos.org/SpecialInterestGroup/Atomic/Download">downstream</a> or <a href="https://wiki.centos.org/SpecialInterestGroup/Atomic/Devel">continuous</a> branches.</p>

<p>With Fedora-based Atomic Hosts, there are two issues. First, Fedora 25 currently ships with an older version of runc. Soon after the <a href="https://bodhi.fedoraproject.org/updates/FEDORA-2017-f4ccc7cb91">updated version</a> gets enough karma, this kubeadm system container will work with Fedora 25&hellip; with SELinux in permissive mode. I’m trying to track down why this is necessary with Fedora but not with CentOS.</p>

<p>To try it for yourself, start out by installing this kubeadm system container, starting the kubelet service, and kicking off <code>kubeadm init</code>:</p>
<pre class="highlight plaintext"><code># atomic install --system --name kubelet docker.io/jasonbrooks/kubeadm&#x000A;# systemctl start kubelet&#x000A;# kubeadm init --skip-preflight-checks --pod-network-cidr=10.254.0.0/16&#x000A;</code></pre>

<p>Once the <code>kubeadm init</code> completes, you’ll need to follow the directions on screen to configure kubectl:</p>
<pre class="highlight plaintext"><code>$ sudo cp /etc/kubernetes/admin.conf $HOME/&#x000A;$ sudo chown $(id -u):$(id -g) $HOME/admin.conf&#x000A;$ export KUBECONFIG=$HOME/admin.conf&#x000A;</code></pre>

<p>Next we need to configure a networking plugin, along with RBAC rules. I&rsquo;ve modified the the <code>kube-flannel.yml</code> file below to work with SELinux:</p>
<pre class="highlight plaintext"><code>$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml&#x000A;$ kubectl apply -f https://raw.githubusercontent.com/jasonbrooks/flannel/support-selinux-kube/Documentation/kube-flannel.yml&#x000A;</code></pre>

<p>Assuming you want your master to do double-duty as a node for an all in one setup, run the following command:</p>
<pre class="highlight plaintext"><code>$ kubectl taint nodes --all node-role.kubernetes.io/master-&#x000A;</code></pre>

<p>Once all the pods in the <code>kube-system</code> namespace are up and running, which you can check with <code>kubectl get pods -n kube-system</code>, you can test out your cluster. I like to use this guestbook-go app:</p>
<pre class="highlight plaintext"><code>$ kubectl apply -f https://gist.githubusercontent.com/jasonbrooks/c2cab426c315ec26266ddd2c78aa4b60/raw/f9199348a04d5d65b60ce974992f5fd589b3e1de/guestbookgo.yaml&#x000A;&#x000A;$ kubectl get svc guestbook&#x000A;NAME        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE&#x000A;guestbook   10.106.170.7   &lt;nodes&gt;       3000:32534/TCP   6m&#x000A;</code></pre>

<p>Once all the pods for the guestbook app are are up and running, which you can check with <code>kubectl get pods</code>, you can test out the app by visiting the IP address of your host at the NodePort listed above (in this case 32534).</p>

<p>Running <code>atomic containers list</code> will show that most of the containers running are run via docker, with the exception of the kubelet container, run via runc:</p>
<pre class="highlight plaintext"><code>$ atomic containers list&#x000A;   CONTAINER ID IMAGE                COMMAND              CREATED          STATE     BACKEND    RUNTIME   &#x000A;   81ab52fe1a6a docker.io/redis@sha2 docker-entrypoint.sh 2017-05-10 14:21 running   docker     docker    &#x000A;   cf2561dbd708 docker.io/kubernetes /bin/sh -c /run.sh   2017-05-10 14:21 running   docker     docker    &#x000A;   9cbbea14fc20 gcr.io/google_contai ./guestbook          2017-05-10 14:21 running   docker     docker    &#x000A;   b8b91e19260e gcr.io/google_contai /pause               2017-05-10 14:21 running   docker     docker    &#x000A;   e68cf8f192ab gcr.io/google_contai /pause               2017-05-10 14:21 running   docker     docker    &#x000A;   f619b1092cfc gcr.io/google_contai /pause               2017-05-10 14:21 running   docker     docker    &#x000A;   5d9b7966a292 gcr.io/google_contai /sidecar --v=2 --log 2017-05-10 12:27 running   docker     docker    &#x000A;   f74a7d3da1bd gcr.io/google_contai /dnsmasq-nanny -v=2  2017-05-10 12:27 running   docker     docker    &#x000A;   b6afd95588a9 gcr.io/google_contai /kube-dns --domain=c 2017-05-10 12:27 running   docker     docker    &#x000A;   96a68d5a53d3 gcr.io/google_contai /pause               2017-05-10 12:27 running   docker     docker    &#x000A;   75f981a02222 quay.io/coreos/flann /opt/bin/flanneld -- 2017-05-10 12:27 running   docker     docker    &#x000A;   2c7d8e7af5c2 gcr.io/google_contai /usr/local/bin/kube- 2017-05-10 12:27 running   docker     docker    &#x000A;   180eeb2e9a26 quay.io/coreos/flann /bin/sh -c 'set -e - 2017-05-10 12:26 running   docker     docker    &#x000A;   1c35ebad2642 gcr.io/google_contai /pause               2017-05-10 12:26 running   docker     docker    &#x000A;   f09b2e9f60ad gcr.io/google_contai /pause               2017-05-10 12:24 running   docker     docker    &#x000A;   b980f99adec8 gcr.io/google_contai etcd --listen-client 2017-05-10 12:24 running   docker     docker    &#x000A;   8d58bd57c632 gcr.io/google_contai kube-apiserver --kub 2017-05-10 12:24 running   docker     docker    &#x000A;   2ffdb4612a64 gcr.io/google_contai kube-scheduler --add 2017-05-10 12:24 running   docker     docker    &#x000A;   2cabf3d7a924 gcr.io/google_contai kube-controller-mana 2017-05-10 12:23 running   docker     docker    &#x000A;   a5a8d203f7e7 gcr.io/google_contai /pause               2017-05-10 12:23 running   docker     docker    &#x000A;   d25bc2c3808a gcr.io/google_contai /pause               2017-05-10 12:23 running   docker     docker    &#x000A;   d3191cdaef80 gcr.io/google_contai /pause               2017-05-10 12:23 running   docker     docker    &#x000A;   ef0d198666bc gcr.io/google_contai /pause               2017-05-10 12:23 running   docker     docker    &#x000A;   kubelet      docker.io/jasonbrook /usr/bin/launch.sh   2017-05-10 12:22 running   ostree     runc&#x000A;</code></pre>

<p>kubeadm is still considered beta for now, and there aren&rsquo;t official Fedora or CentOS packages , so the system container I made pulls that packages directly from the upstream. Once kubeadm is considered GA, I expect to see it available in Fedora-packaged form.</p>

</section>
<footer class='post-meta'>
<div class='tags'>
Tags:
<ul class='taglist'>
<li><a href="/blog/tag/atomic/">atomic</a></li>
<li><a href="/blog/tag/centos/">centos</a></li>
<li><a href="/blog/tag/fedora/">fedora</a></li>
<li><a href="/blog/tag/kubeadm/">kubeadm</a></li>
<li><a href="/blog/tag/kubernetes/">kubernetes</a></li>
</ul>
</div>

<p>
<small>Share this post:</small>
<span class='btn-group' id='social_share'>
<a class='btn btn-default btn-xs' href='http://twitter.com/share?text=yourtext&amp;url=http://www.projectatomic.io/blog/2017/05/testing-system-containerized-kubeadm/' role='button' title='Twitter'>
<i class='fa fa-twitter'></i>
Twitter
</a>
<a class='btn btn-default btn-xs' href='http://www.facebook.com/sharer.php?u=http://www.projectatomic.io/blog/2017/05/testing-system-containerized-kubeadm/' role='button' title='Facebook'>
<i class='fa fa-facebook'></i>
Facebook
</a>
<a class='btn btn-default btn-xs' href='https://plus.google.com/share?url=http://www.projectatomic.io/blog/2017/05/testing-system-containerized-kubeadm/' role='button' title='Google'>
<i class='fa fa-google-plus'></i>
Google+
</a>
</span>
</p>

</footer>
</article>

{{end}}